{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c1ec39",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook demonstrates how to predict stock closing prices using a neural network with PyTorch.  \n",
    "It walks through the full workflow for time series forecasting on financial data, including:\n",
    "\n",
    "- **Data Loading and Visualization:**  \n",
    "  Downloads historical stock data, displays it using pandas, and visualizes the closing prices over time.\n",
    "\n",
    "- **Data Preprocessing:**  \n",
    "  Scales the closing price data, then creates sliding windows of sequential data to prepare it for time series modeling.\n",
    "\n",
    "- **Dataset Preparation:**  \n",
    "  Splits the data into training and test sets, reshaping it for input into an LSTM (Long Short-Term Memory) neural network.\n",
    "\n",
    "- **Model Definition:**  \n",
    "  Defines an LSTM-based neural network architecture for sequence prediction.\n",
    "\n",
    "- **Training:**  \n",
    "  Trains the model using Mean Squared Error loss and the Adam optimizer, running multiple training loops to track performance.\n",
    "\n",
    "- **Evaluation:**  \n",
    "  Makes predictions on the test set, inverse-transforms the results to the original price scale, and calculates the Root Mean Squared Error (RMSE) to assess accuracy.\n",
    "\n",
    "- **Visualization:**  \n",
    "  Plots actual vs. predicted prices and visualizes prediction error over time to help interpret model performance.\n",
    "\n",
    "This end-to-end example provides a practical introduction to time series forecasting with deep learning on financial data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6862db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# This code will check for CUDA, MPS (Apple Silicon), and ROCm (AMD) support\n",
    "# and set the device accordingly.\n",
    "# This is important for PyTorch to use the GPU\n",
    "# if available, otherwise it will default to CPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS device\")\n",
    "elif torch.backends.hip.is_available():\n",
    "    device = torch.device(\"hip\")\n",
    "    print(\"Using AMD ROCm device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No Useable GPU available, fallback to CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ade39c",
   "metadata": {},
   "source": [
    "# Load and Visualize Stock Data\n",
    "\n",
    "This cell sets the stock ticker symbol, loads historical daily stock data for that ticker from Stooq starting from January 1, 2020, and sorts the data by date in ascending order.  \n",
    "It then prints the DataFrame and plots the closing price over time to give a visual overview of the stock's historical performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Ticker to be predicted here.\n",
    "ticker = 'AMD'\n",
    "\n",
    "start = datetime.datetime(2020, 1, 1)\n",
    "end = datetime.datetime.today()\n",
    "# end = datetime.datetime(2025, 1, 31)\n",
    "df = web.DataReader(ticker, 'stooq', start)\n",
    "df = df.sort_index()\n",
    "print(df)\n",
    "\n",
    "df.Close.plot(figsize=(12, 8))\n",
    "plt.title(f'{ticker} Stock Price')\n",
    "plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198dd2ae",
   "metadata": {},
   "source": [
    "# Scaler Explainer\n",
    "\n",
    "Scaler is going to scale the data to fit into a normal distribution with a range of 0 to 1.\n",
    "compaing past results to predict future movements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94efc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[\"Close\"] = scaler.fit_transform(df[[\"Close\"]])\n",
    "df.Close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcaa147",
   "metadata": {},
   "source": [
    "# Create Sliding Windows for Time Series Data\n",
    "\n",
    "This cell creates overlapping sequences (windows) of 30 consecutive closing prices from the normalized stock data.  \n",
    "Each window is used as a sample for the neural network to learn patterns in the time series.  \n",
    "The cell also prints the shape of the resulting data array and shows the first and last window to help you verify the windowing process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 30\n",
    "data = []\n",
    "\n",
    "for i in range(len(df) - seq_length):\n",
    "    data.append(df.Close[i:i + seq_length])\n",
    "data = np.array(data)\n",
    "\n",
    "# Print the shape and a sample\n",
    "# print(\"Shape of data:\", data.shape)\n",
    "# print(\"First window:\", data[0])\n",
    "# print(\"Last window:\", data[-1])\n",
    "\n",
    "# Output to see the data and its shape [dimensions]\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049c6d7",
   "metadata": {},
   "source": [
    "# Reshape Data and Split into Training and Test Sets\n",
    "\n",
    "This cell reshapes the windowed data to add a feature dimension, which is required for LSTM input.  \n",
    "It then splits the data into training and test sets: the first 80% of the windows are used for training, and the last 20% for testing.  \n",
    "For each window, the input (`x_train`/`x_test`) is the first 29 days, and the target (`y_train`/`y_test`) is the 30th day.  \n",
    "All arrays are converted to PyTorch tensors and moved to the selected device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86bbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data to add an extra dimension\n",
    "data = data.reshape(data.shape[0], data.shape[1], 1)\n",
    "\n",
    "# Use the first 80% of the data for training and the last 20% for testing\n",
    "train_size = int(0.8 * len(data))\n",
    "\n",
    "# Models\n",
    "x_train = torch.from_numpy(\n",
    "    data[:train_size, :-1, :]).type(torch.Tensor).to(device)\n",
    "y_train = torch.from_numpy(\n",
    "    data[:train_size, -1, :]).type(torch.Tensor).to(device)\n",
    "\n",
    "# Test Model\n",
    "x_test = torch.from_numpy(\n",
    "    data[train_size:, :-1, :]).type(torch.Tensor).to(device)\n",
    "y_test = torch.from_numpy(\n",
    "    data[train_size:, -1, :]).type(torch.Tensor).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3e332",
   "metadata": {},
   "source": [
    "# Define the LSTM Prediction Model\n",
    "\n",
    "This cell defines the neural network architecture using PyTorch.  \n",
    "It creates a class called `PredictionModel` that uses an LSTM (Long Short-Term Memory) layer to process sequences of stock prices, followed by a fully connected layer to produce the final prediction.  \n",
    "The model is designed to learn patterns in time series data and predict the next closing price based on previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.2):\n",
    "        # Initialize the parent class\n",
    "        super(PredictionModel, self).__init__()\n",
    "\n",
    "        # Initialize the model parameters\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim,\n",
    "                            num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize the hidden state(h) and cell state(c)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(\n",
    "            0), self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(\n",
    "            0), self.hidden_dim, device=device)\n",
    "\n",
    "        # Forward propagate the LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41446e",
   "metadata": {},
   "source": [
    "# Hidden Layers\n",
    "\n",
    "A **hidden layer** in a neural network is any layer between the input and output layers. It’s called “hidden” because you don’t directly interact with it—the network learns what happens there.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- Each hidden layer consists of neurons (nodes) that take inputs, apply weights and biases, and pass the result through an activation function.\n",
    "- The hidden layers allow the network to learn complex patterns and representations from the data.\n",
    "- In deep learning, having more hidden layers (a “deep” network) enables the model to learn more abstract features.\n",
    "\n",
    "**In your LSTM model:**\n",
    "\n",
    "- The LSTM’s `hidden_dim` parameter controls the size of the hidden state vector (how much information each LSTM cell can store).\n",
    "- `num_layers` controls how many LSTM layers (hidden layers) are stacked.\n",
    "\n",
    "**Summary:**  \n",
    "Hidden layers are where the neural network “thinks”—they transform the input step by step, allowing the network to model complex relationships in your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610ce3a",
   "metadata": {},
   "source": [
    "# Set Loss Function and Model/Optimizer Factory\n",
    "\n",
    "This cell sets the loss function for training (Mean Squared Error, which measures how close predictions are to actual values).  \n",
    "It also defines a function that creates a new LSTM model and its optimizer, making it easy to initialize fresh models for each training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set criterion for training\n",
    "# Mean Squared Error Loss, works from the derirviative of the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create the function model & optimizer for training\n",
    "\n",
    "\n",
    "def opt_model():\n",
    "    model = PredictionModel(input_dim=1, hidden_dim=32,\n",
    "                            num_layers=2, output_dim=1).to(device)\n",
    "    # Optimizer - lr = learning rate lower = more accurate but slower\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "# Data Check\n",
    "# print(\"Shape of data (after model definition):\", data.shape)\n",
    "# print(\"First window (after model definition):\", data[0])\n",
    "# print(\"Last window (after model definition):\", data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab352f7",
   "metadata": {},
   "source": [
    "# Train the LSTM Model\n",
    "\n",
    "This cell runs the training loop for the LSTM neural network.  \n",
    "For each run, it creates a new model and optimizer, then trains the model for a set number of epochs.  \n",
    "During training, it predicts on the training data, calculates the loss, performs backpropagation, and updates the model weights.  \n",
    "After each run, it records the training time and final loss, and finally prints the average loss and total training time across all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "runs = 100  # Number of runs\n",
    "run_times = []  # Store the run times\n",
    "num_epochs = 1000  # Number of epochs\n",
    "final_losses = []  # Store the final loss for each run\n",
    "\n",
    "for j in range(runs):\n",
    "    model, optimizer = opt_model()\n",
    "    # Training the model\n",
    "    for i in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        # Model training\n",
    "        y_train_pred = model(x_train)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(y_train_pred, y_train)\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # step to the right direction to optimize the loss\n",
    "    elaspsed = time.time() - start_time\n",
    "    run_times.append(elaspsed)\n",
    "    final_losses.append(loss.item())\n",
    "    print(\n",
    "        f\"Run {j+1}/{runs}, complete | Time: {elaspsed:.3f}s | Final Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Print the final loss for each run\n",
    "avg_loss = sum(final_losses) / len(final_losses)\n",
    "total_time = sum(run_times)  # / len(run_times)\n",
    "\n",
    "print(f\"Average time over {runs} runs: {total_time:.4f}s\")\n",
    "print(f\"Average loss over {runs} runs: {avg_loss:.8f}\")\n",
    "\n",
    "\n",
    "# Nural Nine original print\n",
    "# if i % 25 == 0:  # Print the loss every 25 epochs, % means epoch\n",
    "#            print(i, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bceaa9",
   "metadata": {},
   "source": [
    "# Make Predictions, Inverse Transform, and Evaluate\n",
    "\n",
    "This cell puts the trained model in evaluation mode and generates predictions for the test set.  \n",
    "It then inverse-transforms both the predicted and actual values from the normalized scale back to the original stock price scale.  \n",
    "Finally, it calculates and prints the Root Mean Squared Error (RMSE) for both the training and test sets, giving a quantitative measure of prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# If you get an error like \"AttributeError: 'numpy.ndarray' object has no attribute 'detach'\" here,\n",
    "# it means y_train or y_test is already a NumPy array (not a PyTorch tensor).\n",
    "# This happens if you run this cell (Cell 18) more than once without first re-running the data prep cell (Cell 13),\n",
    "# which recreates y_train and y_test as tensors.\n",
    "#\n",
    "# To fix:\n",
    "# - Always re-run Cell 13 before running this cell.\n",
    "# - OR, add a type check before using .detach(), e.g.:\n",
    "#     if isinstance(y_test, torch.Tensor):\n",
    "#         y_test_np = y_test.detach().cpu().numpy()\n",
    "#     else:\n",
    "#         y_test_np = y_test\n",
    "#     y_test = scaler.inverse_transform(y_test_np)\n",
    "# Do the same for y_train if needed.\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "y_test_pred = model(x_test)  # Make predictions\n",
    "\n",
    "# Reverse the sacling of the data, shift it to the cpu and convert to numpy array\n",
    "\n",
    "# Training Data\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred.detach().cpu().numpy())\n",
    "y_train = scaler.inverse_transform(y_train.detach().cpu().numpy())\n",
    "\n",
    "# Test Data\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred.detach().cpu().numpy())\n",
    "y_test = scaler.inverse_transform(y_test.detach().cpu().numpy())\n",
    "# Calculate the Root Mean Squared Error (RMSE)\n",
    "# is more inline with the prices of the stock\n",
    "train_rmse = root_mean_squared_error(y_train[:, 0], y_train_pred[:, 0])\n",
    "test_rmse = root_mean_squared_error(y_test[:, 0], y_test_pred[:, 0])\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811db8d4",
   "metadata": {},
   "source": [
    "# Plot Actual vs. Predicted Prices and Error Variance\n",
    "\n",
    "This cell visualizes the model's performance.  \n",
    "It plots the actual and predicted stock prices for the test period on the first subplot.  \n",
    "The second subplot shows the prediction error (variance) over time and includes a dashed line for the RMSE value.  \n",
    "This helps you see how closely the model tracks real prices and where prediction errors are largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9226c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Price and Prediction as a line graph\n",
    "# Below the P & P we plot the error variance from the RSME\n",
    "test_start = len(df) - len(y_test)\n",
    "test_index = df.index[-len(y_test):]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "# Grid specification figure (rows, columns)\n",
    "gs = fig.add_gridspec(4, 1)\n",
    "\n",
    "\n",
    "# Axis Specs\n",
    "ax1 = fig.add_subplot(gs[:3, 0])\n",
    "ax2 = fig.add_subplot(gs[3, 0])\n",
    "\n",
    "# Give all of the data from the beginning to the end as the date on the x axis.\n",
    "ax1.plot(df.iloc[-len(y_test):].index, y_test,\n",
    "         color='green', label='Actual Price')\n",
    "ax1.plot(df.iloc[-len(y_test):].index, y_test_pred,\n",
    "         color='red', label='Predected Price')\n",
    "\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "plt.title(f\"{ticker} Stock Price Prediction\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "\n",
    "# Plot the RMSE\n",
    "ax2.axhline(test_rmse, color='black', linestyle='--', label='RMSE')\n",
    "ax2.plot(test_index, abs(y_test - y_test_pred),\n",
    "         color='blue', label='Prediction Variance')\n",
    "\n",
    "ax2.legend()\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Date')\n",
    "plt.title('RSME Variance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
